"""
LLMæ ¸å¿ƒåˆ†æå¼•æ“
è´Ÿè´£å¯¹ç”¨æˆ·åé¦ˆè¿›è¡Œæ·±åº¦åˆ†æï¼ŒåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€æ„å›¾è¯†åˆ«ã€ä¸»é¢˜æå–ç­‰
"""
import json
import asyncio
from typing import Dict, List, Optional, Union
from dataclasses import dataclass
import logging
from abc import ABC, abstractmethod

from openai import AsyncOpenAI
import google.generativeai as genai
from pydantic import BaseModel

from app.core.config import settings
from app.core.database import get_db
from app.models.settings import AppSettings

logger = logging.getLogger(__name__)


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœæ•°æ®ç±»"""
    sentiment: str  # "Positive", "Negative", "Neutral"
    sentiment_score: float  # -1.0 åˆ° 1.0
    intent: str  # "Bug Report", "Feature Request", "Question", "UX Complaint", "Praise"
    topics: List[str]  # ä¸»é¢˜åˆ—è¡¨
    urgency_score: float  # 0.0 åˆ° 1.0
    summary: str  # æ´å¯Ÿæ‘˜è¦
    confidence: float  # åˆ†æç½®ä¿¡åº¦ 0.0 åˆ° 1.0


class SettingsService:
    """è®¾ç½®æœåŠ¡ï¼Œç”¨äºåŠ¨æ€è·å–æ•°æ®åº“ä¸­çš„é…ç½®"""
    
    @staticmethod
    async def get_app_settings() -> AppSettings:
        """ä»æ•°æ®åº“è·å–åº”ç”¨è®¾ç½®"""
        try:
            async for db in get_db():
                settings_doc = await db["settings"].find_one({"_id": "global"})
                if settings_doc and "settings" in settings_doc:
                    return AppSettings.model_validate(settings_doc["settings"])
                else:
                    # è¿”å›é»˜è®¤è®¾ç½®
                    return AppSettings()
        except Exception as e:
            logger.error(f"è·å–è®¾ç½®å¤±è´¥: {e}")
            return AppSettings()


class LLMAnalyzer(ABC):
    """LLMåˆ†æå™¨åŸºç±»"""
    
    def __init__(self):
        self.analysis_prompt = self._build_analysis_prompt()
    
    def _build_analysis_prompt(self) -> str:
        """æ„å»ºåˆ†ææç¤º"""
        return """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”¨æˆ·åé¦ˆåˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä»¥ä¸‹ç”¨æˆ·åé¦ˆå†…å®¹ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚

ç”¨æˆ·åé¦ˆï¼š"{feedback_text}"

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ·±åº¦åˆ†æï¼š

1. **æƒ…æ„Ÿåˆ†æ (sentiment)**: 
   - "Positive": ç”¨æˆ·è¡¨è¾¾æ»¡æ„ã€èµæ‰¬ã€æ„Ÿè°¢ç­‰æ­£é¢æƒ…æ„Ÿ
   - "Negative": ç”¨æˆ·è¡¨è¾¾ä¸æ»¡ã€æŠ±æ€¨ã€æ‰¹è¯„ç­‰è´Ÿé¢æƒ…æ„Ÿ  
   - "Neutral": ç”¨æˆ·è¡¨è¾¾ä¸­æ€§çš„è¯¢é—®ã€å»ºè®®æˆ–é™ˆè¿°

2. **æƒ…æ„Ÿå¼ºåº¦ (sentiment_score)**: -1.0åˆ°1.0çš„æ•°å€¼ï¼Œ-1.0è¡¨ç¤ºæåº¦è´Ÿé¢ï¼Œ1.0è¡¨ç¤ºæåº¦æ­£é¢

3. **åé¦ˆç±»å‹ (intent)**:
   - "Bug Report": æŠ¥å‘Šç³»ç»Ÿé”™è¯¯ã€åŠŸèƒ½å¼‚å¸¸
   - "Feature Request": å»ºè®®æ–°åŠŸèƒ½æˆ–æ”¹è¿›
   - "UX Complaint": ç”¨æˆ·ä½“éªŒé—®é¢˜æŠ•è¯‰
   - "Question": å’¨è¯¢ç±»é—®é¢˜
   - "Praise": è¡¨æ‰¬å’Œèµç¾

4. **å…³é”®è¯æå– (topics)**: 
   - æå–3-5ä¸ªæœ€é‡è¦çš„å…³é”®æ¦‚å¿µè¯æ±‡
   - ä¼˜å…ˆé€‰æ‹©ï¼šåŠŸèƒ½æ¨¡å—åã€æŠ€æœ¯æœ¯è¯­ã€æ ¸å¿ƒé—®é¢˜è¯
   - é¿å…ï¼šè¯­æ°”è¯ã€è¿æ¥è¯ã€è¿‡äºæ³›åŒ–çš„è¯
   - ç¤ºä¾‹ï¼š["ç™»å½•æ¨¡å—", "é¡µé¢åŠ è½½", "æ•°æ®åŒæ­¥", "ç”¨æˆ·ç•Œé¢"]

5. **ç´§æ€¥ç¨‹åº¦ (urgency_score)**: 0.0åˆ°1.0çš„æ•°å€¼
   - 0.0-0.3: ä½ä¼˜å…ˆçº§ï¼ˆå»ºè®®ã€æ”¹è¿›æ„è§ï¼‰
   - 0.4-0.6: ä¸­ç­‰ä¼˜å…ˆçº§ï¼ˆä¸€èˆ¬é—®é¢˜ï¼‰
   - 0.7-0.9: é«˜ä¼˜å…ˆçº§ï¼ˆå½±å“ä½¿ç”¨çš„é—®é¢˜ï¼‰
   - 0.9-1.0: ç´§æ€¥ï¼ˆä¸¥é‡é˜»å¡æ€§é—®é¢˜ï¼‰

6. **é—®é¢˜æ¦‚è¦ (summary)**: ç”¨1-2å¥è¯æ¦‚æ‹¬ç”¨æˆ·çš„æ ¸å¿ƒåé¦ˆç‚¹

7. **åˆ†æç½®ä¿¡åº¦ (confidence)**: 
   - åŸºäºåé¦ˆå†…å®¹çš„æ¸…æ™°åº¦å’Œå®Œæ•´æ€§è¯„ä¼°
   - 0.0-0.5: å†…å®¹æ¨¡ç³Šæˆ–ä¿¡æ¯ä¸è¶³
   - 0.6-0.8: å†…å®¹è¾ƒæ¸…æ™°
   - 0.9-1.0: å†…å®¹éå¸¸æ¸…æ™°æ˜ç¡®

ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š
{{
    "sentiment": "Negative",
    "sentiment_score": -0.7,
    "intent": "Bug Report", 
    "topics": ["ç™»å½•åŠŸèƒ½", "ç½‘ç»œè¶…æ—¶", "é¡µé¢å“åº”"],
    "urgency_score": 0.8,
    "summary": "ç”¨æˆ·åæ˜ ç™»å½•åŠŸèƒ½å­˜åœ¨ç½‘ç»œè¶…æ—¶é—®é¢˜ï¼Œä¸¥é‡å½±å“æ­£å¸¸ä½¿ç”¨ä½“éªŒ",
    "confidence": 0.9
}}
"""
    
    @abstractmethod
    async def analyze_feedback(self, feedback_text: str) -> AnalysisResult:
        """åˆ†æç”¨æˆ·åé¦ˆ"""
        pass
    
    def _parse_analysis_result(self, response_text: str) -> AnalysisResult:
        """è§£æåˆ†æç»“æœ"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                result_dict = json.loads(json_text)
                
                return AnalysisResult(
                    sentiment=result_dict.get("sentiment", "Neutral"),
                    sentiment_score=float(result_dict.get("sentiment_score", 0.0)),
                    intent=result_dict.get("intent", "Question"),
                    topics=result_dict.get("topics", []),
                    urgency_score=float(result_dict.get("urgency_score", 0.0)),
                    summary=result_dict.get("summary", ""),
                    confidence=float(result_dict.get("confidence", 0.5))
                )
            else:
                raise ValueError("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
                
        except Exception as e:
            logger.error(f"è§£æåˆ†æç»“æœå¤±è´¥: {e}, åŸå§‹å›å¤: {response_text}")
            # è¿”å›é»˜è®¤ç»“æœ
            return AnalysisResult(
                sentiment="Neutral",
                sentiment_score=0.0,
                intent="Question",
                topics=[],
                urgency_score=0.0,
                summary="åˆ†æå¤±è´¥",
                confidence=0.0
            )


class OpenAIAnalyzer(LLMAnalyzer):
    """OpenAIåˆ†æå™¨ï¼ˆå…¼å®¹æ–°ç‰ˆAPIï¼‰"""
    
    def __init__(self, api_key: str, model: str = "gpt-4", base_url: str = None):
        super().__init__()
        if not api_key:
            raise ValueError("OpenAI API Keyæœªé…ç½®")
        
        # ä½¿ç”¨æ–°ç‰ˆOpenAIå®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url
        )
        self.model = model
    
    async def analyze_feedback(self, feedback_text: str) -> AnalysisResult:
        """ä½¿ç”¨OpenAIåˆ†æç”¨æˆ·åé¦ˆ"""
        try:
            prompt = self.analysis_prompt.format(feedback_text=feedback_text)
            
            # ä½¿ç”¨æ–°ç‰ˆAPIè°ƒç”¨æ–¹å¼
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”¨æˆ·åé¦ˆåˆ†æå¸ˆã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=500
            )
            
            response_text = response.choices[0].message.content
            return self._parse_analysis_result(response_text)
            
        except Exception as e:
            logger.error(f"OpenAIåˆ†æå¤±è´¥: {e}")
            raise


class GeminiAnalyzer(LLMAnalyzer):
    """Google Geminiåˆ†æå™¨"""
    
    def __init__(self, api_key: str, model: str = "gemini-pro"):
        super().__init__()
        if not api_key:
            raise ValueError("Google API Keyæœªé…ç½®")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model)
    
    async def analyze_feedback(self, feedback_text: str) -> AnalysisResult:
        """ä½¿ç”¨Geminiåˆ†æç”¨æˆ·åé¦ˆ"""
        try:
            prompt = self.analysis_prompt.format(feedback_text=feedback_text)
            
            response = await self.model.generate_content_async(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.1,
                    max_output_tokens=500
                )
            )
            
            response_text = response.text
            return self._parse_analysis_result(response_text)
            
        except Exception as e:
            logger.error(f"Geminiåˆ†æå¤±è´¥: {e}")
            raise


class AnalysisEngine:
    """åˆ†æå¼•æ“ä¸»ç±»"""
    
    def __init__(self):
        self.analyzers = {}
        self.primary_analyzer = None
        self._initialized = False
    
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–åˆ†æå™¨"""
        if self._initialized:
            return
        try:
            app_settings = await SettingsService.get_app_settings()
            self.analyzers = await self._init_analyzers(app_settings)
            self.primary_analyzer = self._get_primary_analyzer()
            self._initialized = True
            logger.info("åˆ†æå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"åˆ†æå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _init_analyzers(self, app_settings: AppSettings) -> Dict[str, LLMAnalyzer]:
        """æ ¹æ®è®¾ç½®åˆå§‹åŒ–å¯ç”¨çš„åˆ†æå™¨"""
        analyzers = {}
        
        if not app_settings.analysis.llm.enabled:
            logger.warning("LLMåˆ†æåŠŸèƒ½å·²ç¦ç”¨")
            return analyzers
        
        llm_settings = app_settings.analysis.llm
        
        # ä¼˜å…ˆä½¿ç”¨theturbo.aié…ç½®
        theturbo_api_key = llm_settings.theturbo_api_key
        theturbo_base_url = llm_settings.theturbo_base_url
        model = llm_settings.model
        
        # æ£€æŸ¥æ˜¯å¦æœ‰theturbo.aié…ç½®
        if theturbo_api_key and theturbo_base_url:
            api_key = theturbo_api_key
            base_url = theturbo_base_url
            logger.info("ä½¿ç”¨theturbo.aié…ç½®è¿›è¡ŒLLMåˆ†æ")
        else:
            # å›é€€åˆ°ä¼ ç»Ÿé…ç½®
            api_key = llm_settings.apiKey
            base_url = llm_settings.base_url
            logger.info("ä½¿ç”¨ä¼ ç»ŸLLMé…ç½®")
        
        if not api_key:
            logger.warning("æœªé…ç½®LLM API Key")
            return analyzers
        
        # æ ¹æ®æ¨¡å‹ç±»å‹åˆå§‹åŒ–å¯¹åº”çš„åˆ†æå™¨
        try:
            logger.info(f"ğŸ”§ å‡†å¤‡åˆå§‹åŒ–åˆ†æå™¨ï¼Œç”¨æˆ·é…ç½®æ¨¡å‹ï¼š{model}")
            
            if model.startswith("gemini") or "gemini" in model.lower():
                # å¯¹äºGeminiæ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨theturbo.aiçš„OpenAIå…¼å®¹æ¥å£
                if theturbo_api_key and theturbo_base_url:
                    analyzers["openai"] = OpenAIAnalyzer(api_key, model, base_url)
                    logger.info(f"âœ… ä½¿ç”¨theturbo.ai OpenAIå…¼å®¹æ¥å£ï¼Œæ¨¡å‹ï¼š{model}ï¼ŒAPIç«¯ç‚¹ï¼š{base_url}")
                else:
                    # å›é€€åˆ°åŸç”ŸGemini API
                    analyzers["gemini"] = GeminiAnalyzer(api_key, model)
                    logger.info(f"âœ… ä½¿ç”¨åŸç”ŸGemini APIï¼Œæ¨¡å‹ï¼š{model}")
            elif model.startswith("gpt") or model.startswith("claude") or "turbo" in model.lower():
                # å¯¹äºOpenAI/Claudeæ¨¡å‹
                analyzers["openai"] = OpenAIAnalyzer(api_key, model, base_url)
                logger.info(f"âœ… OpenAIå…¼å®¹åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹ï¼š{model}ï¼ŒAPIç«¯ç‚¹ï¼š{base_url or 'é»˜è®¤'}")
            else:
                # é»˜è®¤ä½¿ç”¨OpenAIå…¼å®¹åˆ†æå™¨
                analyzers["openai"] = OpenAIAnalyzer(api_key, model, base_url)
                logger.info(f"âœ… ä½¿ç”¨OpenAIå…¼å®¹åˆ†æå™¨ï¼ˆé»˜è®¤ï¼‰ï¼Œæ¨¡å‹ï¼š{model}ï¼ŒAPIç«¯ç‚¹ï¼š{base_url or 'é»˜è®¤'}")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–{model}åˆ†æå™¨å¤±è´¥: {e}")
        
        if not analyzers:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„LLMåˆ†æå™¨ï¼Œè¯·åœ¨å‰ç«¯è®¾ç½®é¡µé¢é…ç½®API Key")
        
        return analyzers
    
    def _get_primary_analyzer(self) -> Optional[LLMAnalyzer]:
        """è·å–ä¸»è¦åˆ†æå™¨"""
        if "openai" in self.analyzers:
            return self.analyzers["openai"]
        elif "gemini" in self.analyzers:
            return self.analyzers["gemini"]
        else:
            return None
    
    async def analyze_single_feedback(self, feedback_text: str) -> AnalysisResult:
        """åˆ†æå•æ¡åé¦ˆ"""
        await self.initialize()
        
        if not self.primary_analyzer:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„åˆ†æå™¨ï¼Œè¯·åœ¨å‰ç«¯è®¾ç½®é¡µé¢é…ç½®LLM API Key")
        
        # å¢å¼ºçš„å†…å®¹æ£€æŸ¥
        if not feedback_text or not feedback_text.strip():
            raise ValueError("åé¦ˆå†…å®¹ä¸èƒ½ä¸ºç©º")
        
        # å¤„ç†Noneå€¼
        if feedback_text is None:
            raise ValueError("åé¦ˆå†…å®¹ä¸ºNone")
        
        # æ–‡æœ¬é•¿åº¦æ£€æŸ¥
        if len(feedback_text) > settings.max_feedback_length:
            feedback_text = feedback_text[:settings.max_feedback_length]
            logger.warning(f"åé¦ˆæ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­åˆ°{settings.max_feedback_length}å­—ç¬¦")
        
        try:
            result = await self.primary_analyzer.analyze_feedback(feedback_text)
            logger.info(f"åé¦ˆåˆ†æå®Œæˆ: æƒ…æ„Ÿ={result.sentiment}, æ„å›¾={result.intent}")
            return result
        except Exception as e:
            logger.error(f"åé¦ˆåˆ†æå¤±è´¥: {e}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨åˆ†æå™¨
            for name, analyzer in self.analyzers.items():
                if analyzer != self.primary_analyzer:
                    try:
                        logger.info(f"å°è¯•ä½¿ç”¨å¤‡ç”¨åˆ†æå™¨: {name}")
                        result = await analyzer.analyze_feedback(feedback_text)
                        return result
                    except Exception as backup_error:
                        logger.error(f"å¤‡ç”¨åˆ†æå™¨{name}ä¹Ÿå¤±è´¥: {backup_error}")
                        continue
            
            # æ‰€æœ‰åˆ†æå™¨éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise RuntimeError(f"æ‰€æœ‰åˆ†æå™¨éƒ½å¤±è´¥ï¼Œæœ€åé”™è¯¯: {e}")
    
    async def analyze_batch_feedback(self, feedback_list: List[str]) -> List[AnalysisResult]:
        """æ‰¹é‡åˆ†æåé¦ˆ"""
        await self.initialize()
        
        tasks = [self.analyze_single_feedback(feedback) for feedback in feedback_list]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"æ‰¹é‡åˆ†æä¸­ç¬¬{i+1}æ¡åé¦ˆå¤±è´¥: {result}")
                # æ·»åŠ é»˜è®¤ç»“æœ
                processed_results.append(AnalysisResult(
                    sentiment="Neutral",
                    sentiment_score=0.0,
                    intent="Question",
                    topics=[],
                    urgency_score=0.0,
                    summary="æ‰¹é‡åˆ†æå¤±è´¥",
                    confidence=0.0
                ))
            else:
                processed_results.append(result)
        
        return processed_results


# åˆ›å»ºå…¨å±€åˆ†æå¼•æ“å®ä¾‹
analysis_engine = AnalysisEngine() 
"""
åŸå§‹æ•°æ®å¤„ç†å™¨
è´Ÿè´£ç¬¬ä¸€å±‚æ•°æ®å¤„ç†ï¼šæ¸…æ´—ã€æ ‡å‡†åŒ–ã€å…¥åº“
"""
import logging
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime
from dataclasses import dataclass
import uuid

from ..data_ingestion.preprocessor import FeedbackPreprocessor
from ...core.database import get_db
from ...models.database import UserFeedback, FeedbackSource

logger = logging.getLogger(__name__)

@dataclass
class RawDataItem:
    """åŸå§‹æ•°æ®é¡¹"""
    data: Dict[str, Any]
    source: str = "spider"
    metadata: Optional[Dict[str, Any]] = None
    created_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

@dataclass 
class ProcessingResult:
    """å¤„ç†ç»“æœ"""
    success: bool
    processed_count: int
    failed_count: int
    feedback_ids: List[str] = None
    errors: List[str] = None
    
    def __post_init__(self):
        if self.feedback_ids is None:
            self.feedback_ids = []
        if self.errors is None:
            self.errors = []

class RawDataProcessor:
    """åŸå§‹æ•°æ®å¤„ç†å™¨
    
    è´Ÿè´£ï¼š
    1. æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
    2. è´¨é‡æ£€æŸ¥å’Œè¿‡æ»¤
    3. æ•°æ®æ ¼å¼ç»Ÿä¸€
    4. å…¥åº“å‰é¢„å¤„ç†
    """
    
    def __init__(self):
        self.preprocessor = FeedbackPreprocessor()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
    async def process_spider_batch(self, 
                                  spider_data: List[Dict[str, Any]], 
                                  task_metadata: Dict[str, Any] = None) -> ProcessingResult:
        """å¤„ç†çˆ¬è™«æ‰¹é‡æ•°æ®"""
        self.logger.info(f"å¼€å§‹å¤„ç† {len(spider_data)} æ¡çˆ¬è™«æ•°æ®")
        
        processed_items = []
        failed_count = 0
        errors = []
        
        # ç¬¬ä¸€æ­¥ï¼šæ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
        for i, raw_item in enumerate(spider_data):
            try:
                # æå–æ ¸å¿ƒå†…å®¹
                content = self._extract_content(raw_item)
                if not content or len(content.strip()) < 5:
                    failed_count += 1
                    filter_reason = f"å†…å®¹è¿‡çŸ­ (é•¿åº¦: {len(content.strip()) if content else 0})"
                    self.logger.warning(f"ğŸš« è¿‡æ»¤æ•°æ®[ç´¢å¼•{i}]: {filter_reason}")
                    self.logger.warning(f"   åŸå§‹æ•°æ®: {raw_item}")
                    errors.append(f"ç´¢å¼•{i}: {filter_reason}")
                    continue
                
                # æ•°æ®é¢„å¤„ç†
                processed_item = self._preprocess_spider_item(raw_item, task_metadata)
                if processed_item:
                    processed_items.append(processed_item)
                    self.logger.debug(f"âœ… æˆåŠŸå¤„ç†æ•°æ®[ç´¢å¼•{i}]: è´¨é‡åˆ†={processed_item['quality_score']:.3f}")
                else:
                    failed_count += 1
                    self.logger.warning(f"ğŸš« é¢„å¤„ç†å¤±è´¥[ç´¢å¼•{i}]: å¯èƒ½æ˜¯è´¨é‡è¯„åˆ†è¿‡ä½")
                    errors.append(f"ç´¢å¼•{i}: é¢„å¤„ç†å¤±è´¥")
                    
            except Exception as e:
                failed_count += 1
                errors.append(f"ç´¢å¼•{i}: {str(e)}")
                self.logger.error(f"å¤„ç†çˆ¬è™«æ•°æ®é¡¹å¤±è´¥: {e}")
        
        # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡å…¥åº“
        if processed_items:
            feedback_ids = await self._batch_save_to_database(processed_items)
            self.logger.info(f"æ•°æ®å¤„ç†å®Œæˆ: æˆåŠŸ {len(feedback_ids)} æ¡ï¼Œå¤±è´¥ {failed_count} æ¡")
            
            # è¯¦ç»†é”™è¯¯æŠ¥å‘Š
            if errors:
                self.logger.warning(f"ğŸš« è¿‡æ»¤è¯¦æƒ… ({failed_count}æ¡):")
                for error in errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                    self.logger.warning(f"   - {error}")
                if len(errors) > 5:
                    self.logger.warning(f"   ... è¿˜æœ‰ {len(errors) - 5} ä¸ªé”™è¯¯")
            
            return ProcessingResult(
                success=True,
                processed_count=len(feedback_ids),
                failed_count=failed_count,
                feedback_ids=feedback_ids,
                errors=errors[:10]  # åªä¿ç•™å‰10ä¸ªé”™è¯¯
            )
        else:
            return ProcessingResult(
                success=False,
                processed_count=0,
                failed_count=failed_count,
                errors=errors[:10]
            )
    
    def _extract_content(self, raw_item: Dict[str, Any]) -> str:
        """æå–æ ¸å¿ƒå†…å®¹"""
        # å°è¯•å¤šç§å¯èƒ½çš„å†…å®¹å­—æ®µ
        content_fields = ['content', 'text', 'body', 'comment', 'review_text']
        
        for field in content_fields:
            content = raw_item.get(field, '')
            if content and isinstance(content, str) and len(content.strip()) > 0:
                return content.strip()
        
        return ""
    
    def _preprocess_spider_item(self, 
                               raw_item: Dict[str, Any], 
                               task_metadata: Dict[str, Any] = None) -> Optional[Dict[str, Any]]:
        """é¢„å¤„ç†å•ä¸ªçˆ¬è™«æ•°æ®é¡¹"""
        try:
            # æå–åŸºç¡€ä¿¡æ¯
            content = self._extract_content(raw_item)
            if not content:
                return None
            
            # æ–‡æœ¬é¢„å¤„ç†
            processed_result = self.preprocessor.preprocess_feedback(
                original_text=content,
                source="spider",
                metadata=task_metadata
            )
            
            # è´¨é‡æ£€æŸ¥
            if processed_result['quality_score'] < 0.1:
                self.logger.warning(f"ğŸš« è¿‡æ»¤ä½è´¨é‡å†…å®¹ (è´¨é‡åˆ†: {processed_result['quality_score']:.3f})")
                self.logger.warning(f"   å†…å®¹: {content[:100]}...")
                self.logger.warning(f"   å¤„ç†æ­¥éª¤: {processed_result.get('processing_steps', [])}")
                self.logger.warning(f"   åŸå› : è´¨é‡è¯„åˆ† {processed_result['quality_score']:.3f} < 0.1")
                return None
            
            # æ„å»ºæ ‡å‡†æ ¼å¼
            processed_item = {
                # åŸºç¡€ä¿¡æ¯ - ä»…ä»çˆ¬è™«æ•°æ®æå–ï¼Œä¸åšAIåˆ†æ
                'content': processed_result['processed_text'],
                'original_content': content,
                'source': self._determine_source(raw_item),
                'source_platform': raw_item.get('source_platform', 'æœªçŸ¥å¹³å°'),
                'original_id': raw_item.get('original_id', str(uuid.uuid4())),
                'url': raw_item.get('url'),
                
                # æ—¶é—´ä¿¡æ¯
                'created_at': self._parse_time(raw_item.get('created_at')),
                'published_at': self._parse_time(raw_item.get('published_at')),
                'crawled_at': datetime.now(),
                
                # ç”¨æˆ·ä¿¡æ¯ï¼ˆåŸå§‹ï¼‰
                'user_info': raw_item.get('user_info', {}),
                
                # äº§å“ä¿¡æ¯ï¼ˆåŸå§‹ï¼‰
                'product_info': raw_item.get('product_info', {}),
                
                # å¹³å°å…ƒæ•°æ®ï¼ˆåŸå§‹ï¼‰
                'platform_metadata': raw_item.get('platform_metadata', {}),
                
                # åœ°ç†ä¿¡æ¯ï¼ˆåŸå§‹ï¼‰
                'geographical_info': raw_item.get('geographical_info', {}),
                
                # é¢„å¤„ç†ç»“æœ
                'language': processed_result['language'],
                'quality_score': processed_result['quality_score'],
                'processing_metadata': {
                    'preprocessed_at': datetime.now(),
                    'processing_steps': processed_result['processing_steps'],
                    'keywords': processed_result['keywords']
                },
                
                # ä»»åŠ¡å…ƒæ•°æ®
                'task_metadata': task_metadata or {},
                
                # å¤„ç†çŠ¶æ€ - æ ‡è®°éœ€è¦åå°å¤„ç†
                'processing_status': {
                    'raw_processed': True,
                    'ai_analyzed': False,
                    'needs_ai_analysis': True,
                    'sentiment_analyzed': False,
                    'priority_calculated': False,
                    'categorized': False
                },
                
                # æ•°æ®è¡€ç¼˜
                'data_lineage': {
                    'source': 'raw_data_processor',
                    'processing_stage': 'raw_preprocessing',
                    'processed_at': datetime.now().isoformat(),
                    'processor_version': '1.0'
                }
            }
            
            return processed_item
            
        except Exception as e:
            self.logger.error(f"é¢„å¤„ç†çˆ¬è™«æ•°æ®é¡¹å¤±è´¥: {e}")
            return None
    
    def _determine_source(self, raw_item: Dict[str, Any]) -> FeedbackSource:
        """ç¡®å®šåé¦ˆæ¥æº"""
        source_type = raw_item.get('source_type', '').lower()
        platform = raw_item.get('platform', '').lower()
        
        if 'ios' in source_type or 'app_store' in source_type:
            return FeedbackSource.APP_STORE
        elif 'android' in source_type or 'google_play' in source_type:
            return FeedbackSource.GOOGLE_PLAY
        elif 'qimai' in raw_item.get('source_platform', '').lower():
            # ä¸ƒéº¦æ•°æ®æ ¹æ®å¹³å°åˆ¤æ–­
            if 'ios' in platform:
                return FeedbackSource.APP_STORE
            elif 'android' in platform:
                return FeedbackSource.GOOGLE_PLAY
        
        return FeedbackSource.INTERNAL
    
    def _parse_time(self, time_value: Any) -> Optional[datetime]:
        """è§£ææ—¶é—´å€¼"""
        if isinstance(time_value, datetime):
            return time_value
        
        if isinstance(time_value, str):
            try:
                # å¸¸è§æ—¶é—´æ ¼å¼
                formats = [
                    "%Y-%m-%d %H:%M:%S",
                    "%Y-%m-%dT%H:%M:%S",
                    "%Y-%m-%dT%H:%M:%S.%f",
                    "%Y-%m-%dT%H:%M:%SZ",
                    "%Y-%m-%d"
                ]
                
                for fmt in formats:
                    try:
                        return datetime.strptime(time_value, fmt)
                    except ValueError:
                        continue
                        
            except Exception as e:
                self.logger.warning(f"æ—¶é—´è§£æå¤±è´¥: {time_value}, {e}")
        
        return None
    
    async def _batch_save_to_database(self, processed_items: List[Dict[str, Any]]) -> List[str]:
        """æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“"""
        feedback_ids = []
        
        for item in processed_items:
            try:
                # åˆ›å»ºUserFeedbackæ–‡æ¡£
                feedback = UserFeedback(
                    content=item['content'],
                    original_content=item['original_content'],
                    source=item['source'],
                    source_platform=item['source_platform'],
                    original_id=item['original_id'],
                    url=item.get('url'),
                    feedback_time=item.get('created_at', datetime.now()),
                    published_at=item.get('published_at'),
                    crawled_at=item.get('crawled_at', datetime.now()),
                    user_info=item.get('user_info', {}),
                    product_info=item.get('product_info', {}),
                    geographical_info=item.get('geographical_info', {}),
                    platform_metadata=item.get('platform_metadata', {}),
                    
                    # é¢„å¤„ç†ä¿¡æ¯
                    language=item['language'],
                    quality_score=item['quality_score'],
                    processing_metadata=item['processing_metadata'],
                    
                    # å¤„ç†çŠ¶æ€
                    processing_status=item['processing_status'],
                    
                    # AIåˆ†æç»“æœï¼ˆå¾…å¡«å……ï¼‰
                    sentiment=None,
                    category=None,
                    keywords=[],
                    priority=None,
                    ai_confidence=None,
                    
                    # ä»»åŠ¡å…ƒæ•°æ®
                    task_metadata=item.get('task_metadata', {}),
                    data_lineage=item['data_lineage'],
                    
                    # å…¼å®¹æ€§å­—æ®µ
                    original_text=item['original_content'],  # è®¾ç½®å…¼å®¹å­—æ®µ
                    processed_text=item['content']          # è®¾ç½®å…¼å®¹å­—æ®µ
                )
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                await feedback.save()
                feedback_ids.append(str(feedback.id))
                
            except Exception as e:
                self.logger.error(f"ä¿å­˜åé¦ˆæ•°æ®å¤±è´¥: {e}")
                continue
        
        return feedback_ids
    
    async def get_processing_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # ç»Ÿè®¡åŸå§‹æ•°æ®å¤„ç†æƒ…å†µ
            total_count = await UserFeedback.count()
            
            # ç»Ÿè®¡å¤„ç†çŠ¶æ€
            raw_processed = await UserFeedback.find(
                {"processing_status.raw_processed": True}
            ).count()
            
            needs_ai_analysis = await UserFeedback.find(
                {"processing_status.needs_ai_analysis": True}
            ).count()
            
            ai_analyzed = await UserFeedback.find(
                {"processing_status.ai_analyzed": True}
            ).count()
            
            return {
                "total_feedback_count": total_count,
                "raw_processed_count": raw_processed,
                "needs_ai_analysis_count": needs_ai_analysis,
                "ai_analyzed_count": ai_analyzed,
                "processing_completion_rate": round(ai_analyzed / total_count * 100, 2) if total_count > 0 else 0
            }
            
        except Exception as e:
            self.logger.error(f"è·å–å¤„ç†ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}

# åˆ›å»ºå…¨å±€å®ä¾‹
raw_data_processor = RawDataProcessor() 
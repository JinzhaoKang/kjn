"""
çˆ¬è™«ç®¡ç†API
æä¾›çˆ¬è™«ä»»åŠ¡çš„åˆ›å»ºã€è¿è¡Œã€ç›‘æ§ç­‰åŠŸèƒ½
"""
from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends, Query, Body
from typing import List, Dict, Any, Optional
from datetime import datetime
from pydantic import BaseModel, Field

from ..services.spider import get_spider_manager, SpiderManager, DataSourcePlatform
from ..services.data_pipeline.data_pipeline_manager import data_pipeline_manager


router = APIRouter(prefix="/api/v1/spider", tags=["çˆ¬è™«ç®¡ç†"])


# è¯·æ±‚æ¨¡å‹
class QimaiTaskRequest(BaseModel):
    """ä¸ƒéº¦çˆ¬è™«ä»»åŠ¡è¯·æ±‚ - iOS"""
    appid: str = Field(..., description="åº”ç”¨ID")
    country: str = Field("cn", description="å›½å®¶ä»£ç ")
    days_back: int = Field(365, description="å›æº¯å¤©æ•°", ge=1, le=3650)
    max_pages: int = Field(100, description="æœ€å¤§æŠ“å–é¡µæ•°", ge=1, le=1000)
    task_name: Optional[str] = Field(None, description="ä»»åŠ¡åç§°")


class QimaiAndroidTaskRequest(BaseModel):
    """ä¸ƒéº¦Androidçˆ¬è™«ä»»åŠ¡è¯·æ±‚"""
    market: str = Field("4", description="åº”ç”¨å¸‚åœºä»£ç ", pattern="^[4679]$")
    max_pages: int = Field(100, description="æœ€å¤§æŠ“å–é¡µæ•°", ge=1, le=1000)
    task_name: Optional[str] = Field(None, description="ä»»åŠ¡åç§°")
    
    class Config:
        schema_extra = {
            "example": {
                "market": "4",
                "max_pages": 100,
                "task_name": "å°ç±³åº”ç”¨å¸‚åœºè¯„è®ºæŠ“å–"
            }
        }


class TaskControlRequest(BaseModel):
    """ä»»åŠ¡æ§åˆ¶è¯·æ±‚"""
    task_id: str = Field(..., description="ä»»åŠ¡ID")


class BatchTaskRequest(BaseModel):
    """æ‰¹é‡ä»»åŠ¡è¯·æ±‚"""
    task_ids: List[str] = Field(..., description="ä»»åŠ¡IDåˆ—è¡¨")


class QuickRunRequest(BaseModel):
    """å¿«é€Ÿè¿è¡Œè¯·æ±‚"""
    platform: str = Field(..., description="å¹³å°åç§°")
    app_id: str = Field(..., description="åº”ç”¨ID")
    max_pages: int = Field(3, description="æœ€å¤§é¡µæ•°", ge=1, le=10)
    enable_ai_analysis: bool = Field(True, description="æ˜¯å¦å¯ç”¨AIåˆ†æ")
    ai_model_id: Optional[str] = Field(None, description="æŒ‡å®šAIæ¨¡å‹ID")
    priority: int = Field(7, description="AIåˆ†æä¼˜å…ˆçº§", ge=1, le=10)


class SpiderConfig(BaseModel):
    """çˆ¬è™«é…ç½®"""
    platform: str = Field(..., description="å¹³å°åç§°")
    settings: Dict[str, Any] = Field(..., description="çˆ¬è™«è®¾ç½®")


class BatchAnalysisRequest(BaseModel):
    """æ‰¹é‡åˆ†æè¯·æ±‚"""
    feedback_ids: List[str] = Field(..., description="åé¦ˆIDåˆ—è¡¨")
    ai_model_id: Optional[str] = Field(None, description="æŒ‡å®šAIæ¨¡å‹ID")
    priority: int = Field(5, description="ä¼˜å…ˆçº§", ge=1, le=10)


# å“åº”æ¨¡å‹
class TaskResponse(BaseModel):
    """ä»»åŠ¡å“åº”"""
    task_id: str
    message: str
    status: str


class TaskStatusResponse(BaseModel):
    """ä»»åŠ¡çŠ¶æ€å“åº”"""
    task_id: str
    status: str
    created_at: str
    started_at: Optional[str]
    completed_at: Optional[str]
    duration: float
    spider_status: Dict[str, Any]
    task_params: Dict[str, Any]
    error: Optional[str]
    result_summary: Optional[Dict[str, Any]]


class SpiderStatisticsResponse(BaseModel):
    """çˆ¬è™«ç»Ÿè®¡å“åº”"""
    total_tasks: int
    running_tasks: int
    idle_tasks: int
    completed_tasks: int
    failed_tasks: int
    total_created: int
    total_completed: int
    total_failed: int
    max_concurrent: int
    registered_platforms: List[str]


# ä¾èµ–æ³¨å…¥
def get_spider_manager_dep() -> SpiderManager:
    """è·å–çˆ¬è™«ç®¡ç†å™¨ä¾èµ–"""
    return get_spider_manager()


@router.post("/qimai/create", response_model=TaskResponse)
async def create_qimai_task(
    request: QimaiTaskRequest,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """åˆ›å»ºä¸ƒéº¦iOSçˆ¬è™«ä»»åŠ¡"""
    try:
        task_id = spider_manager.create_qimai_task(
            appid=request.appid,
            country=request.country,
            days_back=request.days_back,
            max_pages=request.max_pages,
            task_name=request.task_name
        )
        
        return TaskResponse(
            task_id=task_id,
            message="ä¸ƒéº¦iOSçˆ¬è™«ä»»åŠ¡åˆ›å»ºæˆåŠŸ",
            status="created"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/qimai-android/create", response_model=TaskResponse)
async def create_qimai_android_task(
    request: QimaiAndroidTaskRequest,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """åˆ›å»ºä¸ƒéº¦Androidçˆ¬è™«ä»»åŠ¡"""
    try:
        # å¸‚åœºåç§°æ˜ å°„
        market_names = {
            '4': 'å°ç±³åº”ç”¨å¸‚åœº',
            '6': 'åä¸ºåº”ç”¨å¸‚åœº', 
            '7': 'é­…æ—åº”ç”¨å¸‚åœº',
            '8': 'vivoåº”ç”¨å¸‚åœº',
            '9': 'oppoåº”ç”¨å¸‚åœº'
        }
        
        market_name = market_names.get(request.market, f'åº”ç”¨å¸‚åœº({request.market})')
        
        task_id = spider_manager.create_qimai_android_task(
            market=request.market,
            max_pages=request.max_pages,
            task_name=request.task_name or f"{market_name}è¯„è®ºæŠ“å–"
        )
        
        return TaskResponse(
            task_id=task_id,
            message=f"ä¸ƒéº¦Androidçˆ¬è™«ä»»åŠ¡åˆ›å»ºæˆåŠŸ - {market_name}",
            status="created"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"åˆ›å»ºAndroidä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/task/run", response_model=TaskResponse)
async def run_task(
    request: TaskControlRequest,
    background_tasks: BackgroundTasks,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """è¿è¡Œçˆ¬è™«ä»»åŠ¡ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰"""
    try:
        # å¼‚æ­¥è¿è¡Œä»»åŠ¡
        task_id = await spider_manager.run_task_async(request.task_id)
        
        return TaskResponse(
            task_id=task_id,
            message="ä»»åŠ¡å¼€å§‹åå°æ‰§è¡Œ",
            status="running"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"è¿è¡Œä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/task/run-sync")
async def run_task_sync(
    request: TaskControlRequest,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """åŒæ­¥è¿è¡Œçˆ¬è™«ä»»åŠ¡ï¼ˆç­‰å¾…å®Œæˆï¼‰"""
    try:
        result = await spider_manager.run_task(request.task_id)
        
        return {
            "task_id": request.task_id,
            "message": "ä»»åŠ¡æ‰§è¡Œå®Œæˆ",
            "status": result.status.value,
            "data_count": len(result.data),
            "errors_count": len(result.errors),
            "duration": result.metrics.duration if result.metrics else 0,
            "success_rate": result.metrics.success_rate if result.metrics else 0
        }
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"è¿è¡Œä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/task/batch-run")
async def run_multiple_tasks(
    request: BatchTaskRequest,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """æ‰¹é‡è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    try:
        results = await spider_manager.run_multiple_tasks(request.task_ids)
        
        response = []
        for task_id, result in results.items():
            response.append({
                "task_id": task_id,
                "status": result.status.value,
                "data_count": len(result.data),
                "errors_count": len(result.errors),
                "duration": result.metrics.duration if result.metrics else 0
            })
        
        return {
            "message": f"æ‰¹é‡è¿è¡Œ {len(request.task_ids)} ä¸ªä»»åŠ¡å®Œæˆ",
            "results": response
        }
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ‰¹é‡è¿è¡Œå¤±è´¥: {str(e)}")


@router.post("/task/stop", response_model=TaskResponse)
async def stop_task(
    request: TaskControlRequest,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """åœæ­¢çˆ¬è™«ä»»åŠ¡"""
    try:
        spider_manager.stop_task(request.task_id)
        
        return TaskResponse(
            task_id=request.task_id,
            message="ä»»åŠ¡å·²åœæ­¢",
            status="stopped"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"åœæ­¢ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/task/pause", response_model=TaskResponse)
async def pause_task(
    request: TaskControlRequest,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """æš‚åœçˆ¬è™«ä»»åŠ¡"""
    try:
        spider_manager.pause_task(request.task_id)
        
        return TaskResponse(
            task_id=request.task_id,
            message="ä»»åŠ¡å·²æš‚åœ",
            status="paused"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æš‚åœä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/task/resume", response_model=TaskResponse)
async def resume_task(
    request: TaskControlRequest,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """æ¢å¤çˆ¬è™«ä»»åŠ¡"""
    try:
        spider_manager.resume_task(request.task_id)
        
        return TaskResponse(
            task_id=request.task_id,
            message="ä»»åŠ¡å·²æ¢å¤",
            status="running"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ¢å¤ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/task/{task_id}/run", response_model=TaskResponse)
async def run_task_by_id(
    task_id: str,
    background_tasks: BackgroundTasks,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """é€šè¿‡ä»»åŠ¡IDè¿è¡Œçˆ¬è™«ä»»åŠ¡ï¼ˆRESTfulé£æ ¼ï¼‰"""
    try:
        # å¼‚æ­¥è¿è¡Œä»»åŠ¡
        result_task_id = await spider_manager.run_task_async(task_id)
        
        return TaskResponse(
            task_id=result_task_id,
            message="ä»»åŠ¡å¼€å§‹åå°æ‰§è¡Œ",
            status="running"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"è¿è¡Œä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/task/{task_id}/stop", response_model=TaskResponse)
async def stop_task_by_id(
    task_id: str,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """é€šè¿‡ä»»åŠ¡IDåœæ­¢çˆ¬è™«ä»»åŠ¡ï¼ˆRESTfulé£æ ¼ï¼‰"""
    try:
        spider_manager.stop_task(task_id)
        
        return TaskResponse(
            task_id=task_id,
            message="ä»»åŠ¡å·²åœæ­¢",
            status="stopped"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"åœæ­¢ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/task/{task_id}/pause", response_model=TaskResponse)
async def pause_task_by_id(
    task_id: str,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """é€šè¿‡ä»»åŠ¡IDæš‚åœçˆ¬è™«ä»»åŠ¡ï¼ˆRESTfulé£æ ¼ï¼‰"""
    try:
        spider_manager.pause_task(task_id)
        
        return TaskResponse(
            task_id=task_id,
            message="ä»»åŠ¡å·²æš‚åœ",
            status="paused"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æš‚åœä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/task/{task_id}/resume", response_model=TaskResponse)
async def resume_task_by_id(
    task_id: str,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """é€šè¿‡ä»»åŠ¡IDæ¢å¤çˆ¬è™«ä»»åŠ¡ï¼ˆRESTfulé£æ ¼ï¼‰"""
    try:
        spider_manager.resume_task(task_id)
        
        return TaskResponse(
            task_id=task_id,
            message="ä»»åŠ¡å·²æ¢å¤",
            status="running"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ¢å¤ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/task/{task_id}/status", response_model=TaskStatusResponse)
async def get_task_status(
    task_id: str,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        status = spider_manager.get_task_status(task_id)
        
        return TaskStatusResponse(**status)
        
    except Exception as e:
        raise HTTPException(status_code=404, detail=f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")


@router.get("/task/list")
async def list_all_tasks(
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨"""
    try:
        tasks = spider_manager.get_all_tasks()
        
        return {
            "total": len(tasks),
            "tasks": tasks
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get("/task/running")
async def list_running_tasks(
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """è·å–è¿è¡Œä¸­çš„ä»»åŠ¡åˆ—è¡¨"""
    try:
        tasks = spider_manager.get_running_tasks()
        
        return {
            "total": len(tasks),
            "tasks": tasks
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è¿è¡Œä»»åŠ¡å¤±è´¥: {str(e)}")


@router.delete("/task/cleanup")
async def cleanup_completed_tasks(
    keep_days: int = Query(7, description="ä¿ç•™å¤©æ•°", ge=1, le=365),
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡"""
    try:
        cleaned_count = spider_manager.cleanup_completed_tasks(keep_days)
        
        return {
            "message": f"æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸä»»åŠ¡",
            "cleaned_count": cleaned_count,
            "keep_days": keep_days
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¸…ç†ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/statistics", response_model=SpiderStatisticsResponse)
async def get_spider_statistics(
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = spider_manager.get_statistics()
        
        return SpiderStatisticsResponse(**stats)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")


@router.get("/platforms")
async def get_supported_platforms():
    """è·å–æ”¯æŒçš„çˆ¬è™«å¹³å°"""
    try:
        platforms = get_spider_manager().get_supported_platforms()
        return {
            "total": len(platforms),
            "platforms": platforms
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å¹³å°åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get("/ai-models")
async def get_available_ai_models():
    """è·å–å¯ç”¨çš„AIæ¨¡å‹åˆ—è¡¨"""
    try:
        models = data_pipeline_manager.task_engine.get_available_ai_models()
        return {
            "available": len(models) > 0,
            "total": len(models),
            "models": models,
            "default_model": data_pipeline_manager.task_engine.multi_analyzer.default_model if models else None
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–AIæ¨¡å‹å¤±è´¥: {str(e)}")


@router.post("/qimai/quick-run")
async def qimai_quick_run(request: QuickRunRequest):
    """ä¸ƒéº¦æ•°æ®å¿«é€Ÿçˆ¬å–ï¼ˆåŸç‰ˆï¼Œä¸ä½¿ç”¨æ•°æ®ç®¡é“ï¼‰"""
    try:
        logger.info(f"å¯åŠ¨ä¸ƒéº¦çˆ¬è™«: {request.platform}/{request.app_id}")
        
        # æ‰§è¡Œçˆ¬è™«
        result = await get_spider_manager().run_spider(
            spider_name="qimai",
            settings={
                "platform": request.platform,
                "app_id": request.app_id,
                "max_pages": request.max_pages
            }
        )
        
        return {
            "message": "çˆ¬å–å®Œæˆ",
            "spider": "qimai", 
            "platform": request.platform,
            "app_id": request.app_id,
            "result": result,
            "ai_analysis_enabled": False,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ä¸ƒéº¦çˆ¬è™«æ‰§è¡Œå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"çˆ¬è™«æ‰§è¡Œå¤±è´¥: {str(e)}")


@router.post("/qimai/quick-run-with-import")
async def qimai_quick_run_with_import(request: QuickRunRequest):
    """ä¸ƒéº¦æ•°æ®å¿«é€Ÿçˆ¬å–å¹¶å¯¼å…¥åˆ†å±‚å¤„ç†ç®¡é“"""
    try:
        logger.info(f"å¯åŠ¨ä¸ƒéº¦çˆ¬è™«ï¼ˆå¸¦ç®¡é“å¤„ç†ï¼‰: {request.platform}/{request.app_id}")
        
        # æ‰§è¡Œçˆ¬è™«
        spider_result = await get_spider_manager().run_spider(
            spider_name="qimai",
            settings={
                "platform": request.platform,
                "app_id": request.app_id,
                "max_pages": request.max_pages
            }
        )
        
        # å¤„ç†çˆ¬è™«ç»“æœ
        if not spider_result.get("success", False):
            raise HTTPException(status_code=500, detail=f"çˆ¬è™«æ‰§è¡Œå¤±è´¥: {spider_result.get('error')}")
        
        # ä½¿ç”¨æ•°æ®ç®¡é“å¤„ç†çˆ¬è™«æ•°æ®
        raw_data = spider_result.get("data", [])
        if not raw_data:
            return {
                "message": "çˆ¬å–å®Œæˆï¼Œä½†æ²¡æœ‰è·å–åˆ°æ•°æ®",
                "spider": "qimai",
                "platform": request.platform,
                "app_id": request.app_id,
                "data_count": 0,
                "ai_analysis_enabled": False,
                "timestamp": datetime.now().isoformat()
            }
        
        # ä½¿ç”¨æ•°æ®ç®¡é“å¤„ç†
        processing_result = await data_pipeline_manager.process_spider_data(
            spider_data=raw_data,
            task_metadata={
                "spider": "qimai",
                "platform": request.platform,
                "app_id": request.app_id,
                "crawl_time": datetime.now().isoformat()
            },
            enable_ai_analysis=request.enable_ai_analysis
        )
        
        return {
            "message": "çˆ¬å–å’Œæ•°æ®å¤„ç†å®Œæˆ",
            "spider": "qimai",
            "platform": request.platform,
            "app_id": request.app_id,
            "spider_result": {
                "data_count": len(raw_data),
                "execution_time": spider_result.get("execution_time"),
                "pages_crawled": spider_result.get("pages_crawled", 0)
            },
            "pipeline_result": processing_result,
            "ai_analysis": {
                "enabled": request.enable_ai_analysis,
                "model_id": request.ai_model_id,
                "priority": request.priority,
                "queued_tasks": processing_result.get("ai_analysis", {}).get("added_task_count", 0)
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸ƒéº¦çˆ¬è™«+ç®¡é“å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰§è¡Œå¤±è´¥: {str(e)}")


@router.post("/batch-ai-analysis")
async def trigger_batch_ai_analysis(request: BatchAnalysisRequest):
    """æ‰¹é‡è§¦å‘AIåˆ†æ"""
    try:
        # éªŒè¯AIæ¨¡å‹
        if request.ai_model_id:
            available_models = data_pipeline_manager.task_engine.get_available_ai_models()
            model_ids = [m["id"] for m in available_models]
            if request.ai_model_id not in model_ids:
                raise HTTPException(
                    status_code=400, 
                    detail=f"AIæ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨: {request.ai_model_id}. å¯ç”¨æ¨¡å‹: {model_ids}"
                )
        
        # æ·»åŠ AIåˆ†æä»»åŠ¡
        from ..services.data_pipeline.background_task_engine import TaskType
        task_ids = await data_pipeline_manager.task_engine.add_feedback_for_processing(
            feedback_ids=request.feedback_ids,
            task_type=TaskType.FULL_AI_ANALYSIS,
            priority=request.priority,
            model_id=request.ai_model_id
        )
        
        return {
            "message": f"æˆåŠŸæ·»åŠ  {len(task_ids)} ä¸ªAIåˆ†æä»»åŠ¡",
            "feedback_count": len(request.feedback_ids),
            "task_ids": task_ids,
            "ai_model_id": request.ai_model_id,
            "priority": request.priority,
            "queue_info": await data_pipeline_manager.get_processing_queue_info()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡AIåˆ†æå¤±è´¥: {str(e)}")


@router.post("/analyze-all-unprocessed")
async def analyze_all_unprocessed():
    """ä¸€é”®åˆ†ææ‰€æœ‰æœªå¤„ç†çš„åé¦ˆæ•°æ®"""
    try:
        from ..models.database import UserFeedback
        from ..services.data_pipeline.background_task_engine import TaskType
        
        # æŸ¥æ‰¾æ‰€æœ‰æœªåˆ†æçš„æ•°æ®
        unanalyzed_feedback = await UserFeedback.find({
            '$or': [
                {'processing_status.ai_analyzed': {'$ne': True}},
                {'processing_status.ai_analyzed': {'$exists': False}},
                {'sentiment': {'$exists': False}},
                {'category': {'$exists': False}},
                {'priority': {'$exists': False}}
            ]
        }).to_list()
        
        if not unanalyzed_feedback:
            return {
                "message": "æ‰€æœ‰åé¦ˆæ•°æ®éƒ½å·²å®ŒæˆAIåˆ†æ",
                "unanalyzed_count": 0,
                "added_tasks": 0,
                "queue_info": await data_pipeline_manager.get_processing_queue_info()
            }
        
        feedback_ids = [str(feedback.id) for feedback in unanalyzed_feedback]
        
        # æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—
        task_ids = await data_pipeline_manager.task_engine.add_feedback_for_processing(
            feedback_ids=feedback_ids,
            task_type=TaskType.FULL_AI_ANALYSIS,
            priority=8,  # é«˜ä¼˜å…ˆçº§
            model_id=None  # ä½¿ç”¨é»˜è®¤æ¨¡å‹
        )
        
        return {
            "message": f"æˆåŠŸæ·»åŠ  {len(task_ids)} ä¸ªAIåˆ†æä»»åŠ¡ï¼Œå¤„ç† {len(feedback_ids)} æ¡æœªåˆ†æåé¦ˆ",
            "unanalyzed_count": len(feedback_ids),
            "added_tasks": len(task_ids),
            "task_ids": task_ids,
            "queue_info": await data_pipeline_manager.get_processing_queue_info(),
            "analysis_details": {
                "priority": 8,
                "estimated_time": f"{len(feedback_ids) * 5}ç§’",
                "batch_size": len(feedback_ids)
            }
        }
        
    except Exception as e:
        logger.error(f"ä¸€é”®åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¸€é”®åˆ†æå¤±è´¥: {str(e)}")


@router.post("/force-reanalyze-all")
async def force_reanalyze_all():
    """å¼ºåˆ¶é‡æ–°åˆ†ææ‰€æœ‰åé¦ˆæ•°æ®ï¼ˆä½¿ç”¨æ”¹è¿›åçš„AI promptï¼‰"""
    try:
        from ..models.database import UserFeedback
        from ..services.data_pipeline.background_task_engine import TaskType
        
        # æŸ¥æ‰¾æ‰€æœ‰åé¦ˆæ•°æ®ï¼ŒåŒ…æ‹¬å·²åˆ†æçš„
        all_feedback = await UserFeedback.find({}).to_list()
        
        if not all_feedback:
            return {
                "message": "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åé¦ˆæ•°æ®",
                "total_count": 0,
                "added_tasks": 0,
                "queue_info": await data_pipeline_manager.get_processing_queue_info()
            }
        
        feedback_ids = [str(feedback.id) for feedback in all_feedback]
        
        # å…ˆé‡ç½®æ‰€æœ‰æ•°æ®çš„AIåˆ†æçŠ¶æ€ï¼Œå¼ºåˆ¶é‡æ–°åˆ†æ
        await UserFeedback.find({}).update_many({
            '$set': {
                'processing_status.ai_analyzed': False,
                'processing_status.force_reanalyze': True,
                'processing_status.reanalyze_reason': 'improved_prompt_quality'
            }
        })
        
        # æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—ï¼Œä½¿ç”¨æœ€é«˜ä¼˜å…ˆçº§
        task_ids = await data_pipeline_manager.task_engine.add_feedback_for_processing(
            feedback_ids=feedback_ids,
            task_type=TaskType.FULL_AI_ANALYSIS,
            priority=9,  # æœ€é«˜ä¼˜å…ˆçº§
            model_id=None  # ä½¿ç”¨é»˜è®¤æ¨¡å‹
        )
        
        return {
            "message": f"ğŸ”„ å¼ºåˆ¶é‡æ–°åˆ†æå¯åŠ¨ï¼å°†ä½¿ç”¨æ”¹è¿›çš„AI prompté‡æ–°åˆ†æ {len(feedback_ids)} æ¡åé¦ˆæ•°æ®",
            "total_count": len(feedback_ids),
            "added_tasks": len(task_ids),
            "task_ids": task_ids,
            "queue_info": await data_pipeline_manager.get_processing_queue_info(),
            "analysis_details": {
                "priority": 9,
                "estimated_time": f"{len(feedback_ids) * 3}ç§’",
                "batch_size": len(feedback_ids),
                "improvement": "ä½¿ç”¨ä¼˜åŒ–çš„å…³é”®è¯æå–ç®—æ³•å’Œæ›´ç²¾ç¡®çš„æƒ…æ„Ÿåˆ†æ"
            }
        }
        
    except Exception as e:
        logger.error(f"å¼ºåˆ¶é‡æ–°åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¼ºåˆ¶é‡æ–°åˆ†æå¤±è´¥: {str(e)}")


@router.get("/status")
async def get_spider_status():
    """è·å–çˆ¬è™«çŠ¶æ€"""
    try:
        # è·å–çˆ¬è™«ç®¡ç†å™¨çŠ¶æ€
        spider_status = {
            "supported_platforms": get_spider_manager().get_supported_platforms(),
            "manager_initialized": True
        }
        
        # è·å–æ•°æ®ç®¡é“çŠ¶æ€
        pipeline_status = data_pipeline_manager.get_pipeline_status()
        
        # è·å–AIæ¨¡å‹çŠ¶æ€
        ai_models = data_pipeline_manager.task_engine.get_available_ai_models()
        
        return {
            "spider_manager": spider_status,
            "data_pipeline": pipeline_status,
            "ai_capabilities": {
                "models_available": len(ai_models),
                "models": ai_models,
                "analyzer_enabled": pipeline_status["background_engine"]["analyzer_available"]
            },
            "integration_status": "fully_integrated",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")


@router.get("/processing-stats")
async def get_spider_processing_stats():
    """è·å–çˆ¬è™«æ•°æ®å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # è·å–åå°ä»»åŠ¡å¼•æ“çŠ¶æ€
        engine_status = data_pipeline_manager.task_engine.get_engine_status()
        
        return {
            "success": True,
            "data": {
                "queue_size": engine_status["queue_size"],
                "running_tasks": engine_status["running_tasks"],
                "max_concurrent": engine_status["max_concurrent"],
                "engine_running": engine_status["is_running"],
                "analyzer_available": engine_status["analyzer_available"],
                "statistics": engine_status["statistics"]
            }
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}",
            "status_code": 500,
            "timestamp": datetime.now().isoformat()
        }


@router.post("/config")
async def configure_spider(request: SpiderConfig):
    """é…ç½®çˆ¬è™«"""
    try:
        # éªŒè¯å¹³å°
        supported = get_spider_manager().get_supported_platforms()
        if request.platform not in [p["name"] for p in supported]:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„å¹³å°: {request.platform}")
        
        # TODO: å®ç°çˆ¬è™«é…ç½®é€»è¾‘
        return {
            "message": f"çˆ¬è™«é…ç½®æ›´æ–°æˆåŠŸ: {request.platform}",
            "platform": request.platform,
            "settings": request.settings
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é…ç½®çˆ¬è™«å¤±è´¥: {str(e)}")


@router.get("/test-pipeline")
async def test_pipeline_integration():
    """æµ‹è¯•ç®¡é“é›†æˆ"""
    try:
        # æ£€æŸ¥æ•°æ®ç®¡é“çŠ¶æ€
        pipeline_status = data_pipeline_manager.get_pipeline_status()
        
        # æ£€æŸ¥AIåˆ†æå™¨
        ai_models = data_pipeline_manager.task_engine.get_available_ai_models()
        
        # æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
        test_data = [{
            "content": "è¿™ä¸ªåº”ç”¨å¾ˆå¥½ç”¨ï¼Œç•Œé¢è®¾è®¡ä¸é”™ï¼Œä½†æ˜¯åŠ è½½é€Ÿåº¦æœ‰ç‚¹æ…¢",
            "user_info": {"rating": 4},
            "timestamp": datetime.now().isoformat()
        }]
        
        # æµ‹è¯•å¤„ç†ç®¡é“
        test_result = await data_pipeline_manager.process_spider_data(
            spider_data=test_data,
            task_metadata={
                "spider": "test",
                "platform": "test_platform",
                "app_id": "test_app"
            },
            enable_ai_analysis=True
        )
        
        return {
            "pipeline_integration": "success",
            "pipeline_status": pipeline_status["initialized"],
            "ai_analyzer_available": len(ai_models) > 0,
            "test_processing": test_result,
            "capabilities": {
                "raw_processing": True,
                "ai_analysis": len(ai_models) > 0,
                "queue_management": True
            }
        }
        
    except Exception as e:
        return {
            "pipeline_integration": "failed",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


@router.post("/task/{task_id}/import-data")
async def import_spider_data(
    task_id: str,
    enable_ai_analysis: bool = Query(True, description="æ˜¯å¦å¯ç”¨AIåˆ†æ"),
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """å°†çˆ¬è™«æ•°æ®å¯¼å…¥åˆ°ç³»ç»Ÿï¼ˆä½¿ç”¨åˆ†å±‚æ•°æ®ç®¡é“ï¼‰"""
    try:
        # è·å–ä»»åŠ¡çŠ¶æ€
        task_status = spider_manager.get_task_status(task_id)
        
        if task_status["status"] != "completed":
            raise HTTPException(status_code=400, detail="ä»»åŠ¡å°šæœªå®Œæˆï¼Œæ— æ³•å¯¼å…¥æ•°æ®")
        
        # è·å–ä»»åŠ¡ç»“æœ
        task = spider_manager.tasks[task_id]
        if not task.result or not task.result.data:
            raise HTTPException(status_code=400, detail="ä»»åŠ¡æ²¡æœ‰å¯å¯¼å…¥çš„æ•°æ®")
        
        # ä½¿ç”¨æ•°æ®ç®¡é“å¤„ç†
        pipeline_result = await data_pipeline_manager.process_spider_data(
            spider_data=task.result.data,
            task_metadata={
                "spider_task_id": task_id,
                "spider_platform": task.spider.config.platform.value,
                "import_time": datetime.now().isoformat(),
                "task_params": task.task_params
            },
            enable_ai_analysis=enable_ai_analysis
        )
        
        return {
            "message": "çˆ¬è™«æ•°æ®å¯¼å…¥æˆåŠŸï¼ˆåˆ†å±‚å¤„ç†ï¼‰",
            "task_id": task_id,
            "pipeline_result": {
                "success": pipeline_result.success,
                "total_input": pipeline_result.total_input,
                "raw_processed": pipeline_result.raw_processed,
                "raw_failed": pipeline_result.raw_failed,
                "ai_tasks_created": pipeline_result.ai_tasks_created,
                "processing_time": pipeline_result.processing_time
            },
            "feedback_ids": pipeline_result.feedback_ids,
            "processing_layers": {
                "layer_1_raw_processing": "completed",
                "layer_2_ai_analysis": "queued" if enable_ai_analysis else "skipped"
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯¼å…¥æ•°æ®å¤±è´¥: {str(e)}")


@router.get("/task/{task_id}/preview")
async def preview_spider_data(
    task_id: str,
    limit: int = Query(10, description="é¢„è§ˆæ•°é‡", ge=1, le=100),
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """é¢„è§ˆçˆ¬è™«æ•°æ®"""
    try:
        # è·å–ä»»åŠ¡
        if task_id not in spider_manager.tasks:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        task = spider_manager.tasks[task_id]
        
        if not task.result:
            raise HTTPException(status_code=400, detail="ä»»åŠ¡å°šæœªå®Œæˆæˆ–æ²¡æœ‰ç»“æœ")
        
        # é¢„è§ˆæ•°æ®
        preview_data = task.result.data[:limit]
        
        return {
            "task_id": task_id,
            "total_count": len(task.result.data),
            "preview_count": len(preview_data),
            "preview_data": preview_data,
            "task_info": {
                "status": task.status.value,
                "platform": task.spider.config.platform.value,
                "created_at": task.created_at.isoformat(),
                "completed_at": task.completed_at.isoformat() if task.completed_at else None
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é¢„è§ˆæ•°æ®å¤±è´¥: {str(e)}")


# å¿«æ·åˆ›å»ºå’Œè¿è¡Œ
@router.post("/qimai/quick-run")
async def quick_run_qimai_spider(
    request: QimaiTaskRequest,
    background_tasks: BackgroundTasks,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """å¿«é€Ÿåˆ›å»ºå¹¶è¿è¡Œä¸ƒéº¦çˆ¬è™«"""
    try:
        # åˆ›å»ºä»»åŠ¡
        task_id = spider_manager.create_qimai_task(
            appid=request.appid,
            country=request.country,
            days_back=request.days_back,
            max_pages=request.max_pages,
            task_name=request.task_name
        )
        
        # å¼‚æ­¥è¿è¡Œä»»åŠ¡
        await spider_manager.run_task_async(task_id)
        
        return {
            "task_id": task_id,
            "message": "ä¸ƒéº¦çˆ¬è™«ä»»åŠ¡åˆ›å»ºå¹¶å¼€å§‹æ‰§è¡Œ",
            "status": "running",
            "app_info": {
                "appid": request.appid,
                "country": request.country,
                "days_back": request.days_back,
                "max_pages": request.max_pages
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"å¿«é€Ÿè¿è¡Œå¤±è´¥: {str(e)}")


@router.post("/qimai/quick-run-with-import")
async def quick_run_qimai_spider_with_import(
    request: QimaiTaskRequest,
    background_tasks: BackgroundTasks,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """å¿«é€Ÿåˆ›å»ºå¹¶è¿è¡Œä¸ƒéº¦çˆ¬è™«ï¼Œè‡ªåŠ¨å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“"""
    try:
        # åˆ›å»ºä»»åŠ¡
        task_id = spider_manager.create_qimai_task(
            appid=request.appid,
            country=request.country,
            days_back=request.days_back,
            max_pages=request.max_pages,
            task_name=request.task_name
        )
        
        # åŒæ­¥è¿è¡Œä»»åŠ¡å¹¶ç­‰å¾…å®Œæˆ
        result = await spider_manager.run_task(task_id)
        
        # è‡ªåŠ¨å¯¼å…¥æ•°æ®ï¼ˆä½¿ç”¨æ•°æ®ç®¡é“ï¼‰
        if result.data:
            pipeline_result = await data_pipeline_manager.process_spider_data(
                spider_data=result.data,
                task_metadata={
                    "spider_task_id": task_id,
                    "spider_platform": spider_manager.tasks[task_id].spider.config.platform.value,
                    "import_time": datetime.now().isoformat(),
                    "app_info": {
                        "appid": request.appid,
                        "country": request.country
                    }
                },
                enable_ai_analysis=True
            )
            
            return {
                "task_id": task_id,
                "message": "ä¸ƒéº¦çˆ¬è™«ä»»åŠ¡å®Œæˆå¹¶æˆåŠŸå¯¼å…¥æ•°æ®",
                "status": "completed_with_import",
                "app_info": {
                    "appid": request.appid,
                    "country": request.country,
                    "days_back": request.days_back,
                    "max_pages": request.max_pages
                },
                "spider_result": {
                    "data_count": len(result.data),
                    "errors_count": len(result.errors),
                    "duration": result.metrics.duration if result.metrics else 0,
                    "success_rate": result.metrics.success_rate if result.metrics else 0
                },
                "pipeline_result": {
                    "raw_processed": pipeline_result.raw_processed,
                    "raw_failed": pipeline_result.raw_failed,
                    "ai_tasks_created": pipeline_result.ai_tasks_created,
                    "processing_time": pipeline_result.processing_time,
                    "success_rate": round(pipeline_result.raw_processed / pipeline_result.total_input * 100, 2) if pipeline_result.total_input > 0 else 0
                }
            }
        else:
            return {
                "task_id": task_id,
                "message": "ä¸ƒéº¦çˆ¬è™«ä»»åŠ¡å®Œæˆï¼Œä½†æ²¡æœ‰æ•°æ®å¯å¯¼å…¥",
                "status": "completed_no_data",
                "app_info": {
                    "appid": request.appid,
                    "country": request.country,
                    "days_back": request.days_back,
                    "max_pages": request.max_pages
                },
                "spider_result": {
                    "data_count": 0,
                    "errors_count": len(result.errors),
                    "duration": result.metrics.duration if result.metrics else 0
                }
            }
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"å¿«é€Ÿè¿è¡Œå’Œå¯¼å…¥å¤±è´¥: {str(e)}")


@router.post("/qimai-android/quick-run-with-import")
async def quick_run_qimai_android_spider_with_import(
    request: QimaiAndroidTaskRequest,
    background_tasks: BackgroundTasks,
    spider_manager: SpiderManager = Depends(get_spider_manager_dep)
):
    """å¿«é€Ÿåˆ›å»ºå¹¶è¿è¡Œä¸ƒéº¦Androidçˆ¬è™«ï¼Œè‡ªåŠ¨å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“"""
    try:
        # å¸‚åœºåç§°æ˜ å°„
        market_names = {
            '4': 'å°ç±³åº”ç”¨å¸‚åœº',
            '6': 'åä¸ºåº”ç”¨å¸‚åœº', 
            '7': 'é­…æ—åº”ç”¨å¸‚åœº',
            '8': 'vivoåº”ç”¨å¸‚åœº',
            '9': 'oppoåº”ç”¨å¸‚åœº'
        }
        
        market_name = market_names.get(request.market, f'åº”ç”¨å¸‚åœº({request.market})')
        
        # åˆ›å»ºä»»åŠ¡
        task_id = spider_manager.create_qimai_android_task(
            market=request.market,
            max_pages=request.max_pages,
            task_name=request.task_name or f"{market_name}è¯„è®ºæŠ“å–"
        )
        
        # åŒæ­¥è¿è¡Œä»»åŠ¡å¹¶ç­‰å¾…å®Œæˆ
        result = await spider_manager.run_task(task_id)
        
        # è‡ªåŠ¨å¯¼å…¥æ•°æ®ï¼ˆä½¿ç”¨æ•°æ®ç®¡é“ï¼‰
        if result.data:
            pipeline_result = await data_pipeline_manager.process_spider_data(
                spider_data=result.data,
                task_metadata={
                    "spider_task_id": task_id,
                    "spider_platform": spider_manager.tasks[task_id].spider.config.platform.value,
                    "import_time": datetime.now().isoformat(),
                    "market_info": {
                        "market": request.market,
                        "market_name": market_name
                    }
                },
                enable_ai_analysis=True
            )
            
            return {
                "task_id": task_id,
                "message": f"ä¸ƒéº¦Androidçˆ¬è™«ä»»åŠ¡å®Œæˆå¹¶æˆåŠŸå¯¼å…¥æ•°æ® - {market_name}",
                "status": "completed_with_import",
                "market_info": {
                    "market": request.market,
                    "market_name": market_name,
                    "max_pages": request.max_pages
                },
                "spider_result": {
                    "data_count": len(result.data),
                    "errors_count": len(result.errors),
                    "duration": result.metrics.duration if result.metrics else 0,
                    "success_rate": result.metrics.success_rate if result.metrics else 0
                },
                "pipeline_result": {
                    "raw_processed": pipeline_result.raw_processed,
                    "raw_failed": pipeline_result.raw_failed,
                    "ai_tasks_created": pipeline_result.ai_tasks_created,
                    "processing_time": pipeline_result.processing_time,
                    "success_rate": round(pipeline_result.raw_processed / pipeline_result.total_input * 100, 2) if pipeline_result.total_input > 0 else 0
                }
            }
        else:
            return {
                "task_id": task_id,
                "message": f"ä¸ƒéº¦Androidçˆ¬è™«ä»»åŠ¡å®Œæˆï¼Œä½†æ²¡æœ‰æ•°æ®å¯å¯¼å…¥ - {market_name}",
                "status": "completed_no_data",
                "market_info": {
                    "market": request.market,
                    "market_name": market_name,
                    "max_pages": request.max_pages
                },
                "spider_result": {
                    "data_count": 0,
                    "errors_count": len(result.errors),
                    "duration": result.metrics.duration if result.metrics else 0
                }
            }
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"å¿«é€Ÿè¿è¡Œå’Œå¯¼å…¥å¤±è´¥: {str(e)}")


@router.get("/health")
async def spider_health_check():
    """çˆ¬è™«ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    try:
        spider_manager = get_spider_manager()
        stats = spider_manager.get_statistics()
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "spider_manager": {
                "total_tasks": stats["total_tasks"],
                "running_tasks": stats["running_tasks"],
                "max_concurrent": stats["max_concurrent"]
            },
            "supported_platforms": list(spider_manager.spider_registry.keys())
        }
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        } 